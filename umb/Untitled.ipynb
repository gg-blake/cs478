{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb23c094-1fb1-4bf4-aafa-ec7c00fe0213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import json\n",
    "from torch.optim import adamw\n",
    "import tiktoken\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_size, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.query_weights = nn.Linear(embed_size, head_size, bias=False, device=device)\n",
    "        self.key_weights = nn.Linear(embed_size, head_size, bias=False, device=device)\n",
    "        self.value_weights = nn.Linear(embed_size, head_size, bias=False, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        B, T, C = embeddings.shape\n",
    "        # Queries store the information of what other embeddings have in a particular block\n",
    "        query = self.query_weights(embeddings)\n",
    "        # Keys store the information that a particular embedding has relative to other embeddings in a block\n",
    "        key = self.key_weights(embeddings)\n",
    "        # By multiplying the keys and queries together, we can allow the embeddings to influence the meaning of other embeddings in the block\n",
    "        # We need to sqrt(embed_size) to ensure the softmax of wei doesn't get to spiky\n",
    "        wei = query @ key.transpose(-2, -1) * self.embed_size**-0.5 \n",
    "        # When training a model, we don't want embeddings that are ahead of an embedding in a block to send information to it (its like cheating in a test)\n",
    "        # So we will apply a mask to wei\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        # Then we apply a softmax to make the output on interval [0,1)\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "        # We don't apply the embeddings directly to wei but instead we apply another backpropagatable linear layer to the embeddings (called value) and then apply wei\n",
    "        value = self.value_weights(embeddings)\n",
    "        return wei @ value\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, head_size, head_count, block_size, dropout):\n",
    "        super().__init__()\n",
    "        # Multiheaded attention (batched attention calculation)\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_size, head_size // head_count, block_size, dropout) for _ in range(head_count)])\n",
    "        # Linear projection of outcome of multiheaded attention layer\n",
    "        self.proj = nn.Linear(embed_size, embed_size, device=device)\n",
    "        # Randomly zeros out some of the data to prevent overfitting in training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Simple multilayered perceptron\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size, device=device),\n",
    "            self.dropout\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_size, device=device)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_size, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We want to ensure that our nodes across each batch dimension have mean = 0 and standard deviation = 0 before feeding to the multiheaded attention layer\n",
    "        # So we want to apply whats called layer normalization\n",
    "        # Here is the pytorch documentation: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html (LayerNorm)\n",
    "        layer_norm = self.layer_norm1(x)\n",
    "        # Both the multiheaded attention layer and feed forward layer add the in features of the layer to the out features\n",
    "        # This is what is referred to as residual connections, and it solves an issue where increasingly deep networks become hard to train/optimize\n",
    "        # The paper discussing the benefits of this can be found here: https://arxiv.org/abs/1512.03385 (Deep Residual Learning for Image Recognition)\n",
    "        x = x + self.head_forward(layer_norm)\n",
    "        # We also want to apply layer normalization to our attention output before passing it to the feed forward layer\n",
    "        # In the original Attention is All You Need paper, layer normalization comes after each layer, but better results come from doing pre-layer normalization\n",
    "        layer_norm = self.layer_norm2(x)\n",
    "        # Once all the nodes in the head have their individual attention scores, we need to train the nodes to compute their attention scores individually\n",
    "        # This is why we feed the data into a multilayered perceptron, which will allow the model to recognize patterns in the data\n",
    "        x = x + self.linear_forward(layer_norm)\n",
    "        return x\n",
    "\n",
    "    def head_forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # We want to recombine the outcomes together so we must project it to a layer of the right dimensions \n",
    "        # (head_count x embed_size x [embed_size // head_count]) -> (embed_size x embed_size)\n",
    "        out = self.dropout(out)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "    \n",
    "    def linear_forward(self, x):\n",
    "        return self.ffwd(x)\n",
    "    \n",
    "    \n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size, block_size, learning_rate, steps, head_count, layer_count, dropout):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.steps = steps\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_size, device=device)\n",
    "        self.positional_embeddings = nn.Embedding(block_size, embedding_size, device=device)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embedding_size, embedding_size, head_count, block_size, dropout) for _ in range(layer_count)])\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size, device=device)\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size, bias=False, device=device)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_idx = self.token_embeddings(idx)\n",
    "        positional_idx = self.positional_embeddings(torch.arange(T, device=device))\n",
    "        \n",
    "        logits = token_idx + positional_idx\n",
    "        logits = self.blocks(logits)\n",
    "        logits = self.lm_head(logits)\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "    def train_model(self, tokens, eval_iters=200, training_val_ratio=0.8, loss_report_interval=500):\n",
    "        training_tokens = tokens[:floor(len(tokens)*training_val_ratio)]\n",
    "        validation_tokens = tokens[floor(len(tokens)*training_val_ratio):]\n",
    "        optimizer = adamw.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        for step in tqdm(range(self.steps)):\n",
    "            optimizer.zero_grad()\n",
    "            s, t = sample(training_tokens, 4, 8)\n",
    "            logits, loss = lm(s, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % loss_report_interval == 0:\n",
    "                losses = self.estimate_loss(eval_iters, training_tokens, validation_tokens)\n",
    "                print(f\"step {step}: train loss {losses[0]:.4f}, val loss {losses[1]:.4f}\")\n",
    "                \n",
    "                #print(f\"step {step}: train loss {loss:.4f}, val loss {loss:.4f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, eval_iters, training_data, validation_data):\n",
    "        out = {}\n",
    "        # Disable dropout and layer normalization before model validation\n",
    "        self.eval()\n",
    "        for i, split in enumerate([training_data, validation_data]):\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = sample(split, self.batch_size, self.block_size)\n",
    "                logits, loss = self(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[i] = losses.mean()\n",
    "        # Enable dropout and layer normalization after model validation\n",
    "        self.train()\n",
    "        return out\n",
    "\n",
    "\n",
    "def sample(data, batch_size, block_size):\n",
    "    starting_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    sample = torch.stack([data[start_idx:start_idx+block_size] for start_idx in starting_indices])\n",
    "    target = torch.stack([data[start_idx+1:start_idx+block_size+1] for start_idx in starting_indices])\n",
    "    return sample, target\n",
    "\n",
    "def useTiktoken(filename):\n",
    "    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "    assert tokenizer.decode(tokenizer.encode(\"hello world\")) == \"hello world\"\n",
    "    with open(filename) as f:\n",
    "        tokens = torch.tensor(tokenizer.encode(f.read()), dtype=torch.long, device=device)\n",
    "\n",
    "    return tokenizer, tokens, tokenizer.n_vocab\n",
    "\n",
    "def useLocal(filename):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.load(\"tokenizer_models/umb100k-1.model\")\n",
    "    assert tokenizer.decode(tokenizer.encode(\"hello world\")) == \"hello world\"\n",
    "    with open(filename) as f:\n",
    "        tokens = torch.tensor(tokenizer.encode(f.read()), dtype=torch.long, device=device)\n",
    "\n",
    "    return tokenizer, tokens, len(tokenizer._vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3694ef-4285-4ccb-a23c-facf7c56e14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
