Chimera Scripts - UMass Boston honeypot link Skip to Main Navigation Skip to Search Skip to Main Content Skip to Footer Links UMass Toggle Main Menu Admissions Admissions Overview Visit UMass Boston Financial Aid First-Year Students Transfer Students Graduate Students International Students Veterans Academics Academics Overview Majors & Programs Online Learning Colleges & Schools Advising Academic Calendar Healey Library Global Programs Study Abroad Fellowships Campus Life Campus Life Overview Student Groups & Activities Housing & Dining Health & Wellness Diversity & Inclusion Safety & Security Orientation & New Students Events Research Research Overview Community-Driven Research Recognizing Excellence Student Research Centers & Institutes Core Facilities Research & Sponsored Programs About About Overview Leadership & Administration Mission & Vision Facts & Figures Accreditation & Rankings History of UMass Boston Student Consumer Information News Athletics Athletics Overview Recreation at UMass Boston Current Students Parents & Families Faculty & Staff Alumni Toggle Search Search UMass Boston Current Students Parents & Families Faculty & Staff Alumni Home Information Technology Research Computing High Performance Computing Chimera Scripts Menu menu Menu Applications High Performance Computing Access Request Chimera Chimera Scheduler Chimera Scripts Gibbs Gibbs Scheduler Gibbs Scripts Research Computing Knowledge Base Research Computing Advisory Committee Services Staff Chimera Scripts Jobs can be submitted using the 'sbatch' command, generally followed by the name of the submission script.  Common line options are included in the sample scripts. A full listing can be found in the man pages ('man sbatch'). Status of queued or running jobs can be obtained via the 'squeue' command. Download the sample script below (you need to remove the txt file extension after you download it). When you are logged in on Chimera, you can copy the sample script from /share/apps/training/sample_scripts/run_scavenger.sh to your working directory and modify as needed. Interactive job submission can also be used. Sample Chimera Submission Script #!/bin/bash # Sample slurm submission script for the Chimera # compute cluster # Lines beginning with # are comments, and will be ignored by # the interpreter.  Lines beginning with #SBATCH are # directives to the scheduler.  These in turn can be # commented out by adding a second # (e.g. ##SBATCH lines # will not be processed by the scheduler). # # # set name of job #SBATCH --job-name=slurm-sample # # set the number of processors/tasks needed ##SBATCH -n 4 # for hyperthreaded,shared memory jobs, set 1 task, 1 node, # and set --cpus-per-task to total number of threads #SBATCH -n 1 #SBATCH --cpus-per-task=4 # set the number of Nodes needed.  Set to 1 for shared # memory jobs #SBATCH -N 1 #set an account to use #if not used then default will be used # for scavenger users, use this format: ##SBATCH --account=pi_first.last # for contributing users, use this format: ##SBATCH --account=<deptname|lastname> # set max wallclock time  DD-HH:MM:SS #SBATCH --time=00-10:00:00 # set a memory request #SBATCH --mem=1gb # Set filenames for stdout and stderr.  %j can be used # for the jobid. # see "filename patterns" section of the sbatch man page for # additional options #SBATCH --error=%x-%j.err #SBATCH --output=%x-%j.out # # set the partition where the job will run.  Multiple partitions can # be specified as a comma separated list # Use command "sinfo" to get the list of partitions ##SBATCH --partition=Intel6240 #SBATCH --partition=Intel6240,Intel6248,DGXA100 # restricting inheritance of environment variables is # required for chimera12 and 13: # if this option is used, source /etc/profile below. #SBATCH --export=HOME #Optional # mail alert at start, end and/or failure of execution # see the sbatch man page for other options ##SBATCH --mail-type=ALL # send mail to this address ##SBATCH --mail-user=first.last@umb.edu # Put your job commands here, including loading any needed # modules or diagnostic echos. # this job simply reports the hostname and sleeps for # two minutes # source the local profile.  This is recommended in # conjunction with the --export=HOME or --export=NONE # sbatch options. . /etc/profile echo "using $SLURM_CPUS_ON_NODE CPUs" echo `date` hostname sleep 120 # Diagnostic/Logging Information echo "Finish Run" echo "end time is `date`" Interactive Use something like the following commands to submit a interactive jobs on chimera : srun -n 1 -N 1 --cpus-per-task=4 -p Intel6126 -t 01:00:00 --pty /bin/bash srun -n 1 -N 1 --cpus-per-task=16 -p DGXA100 -t 01:00:00 --gres=gpu:1 --export=NONE --pty /bin/bash Modify the various parameters as appropriate.  See 'man sbatch' for information on command line options. When submitting to the GPU nodes:  add "--gres=gpu:1 --export=NONE" to the command above,  and after getting a prompt on the compute node,  issue the command "source /etc/profile". Back to top IT Research Computing Healey Library, Lower Level UMass Boston 100 Morrissey Blvd. Boston, MA 02125 Book a Consultation 617.287.5399 It-rc@umb.edu UMass 100 Morrissey Blvd. Boston, MA 02125 617.287.5000 Contact UMass Boston Directory Employment Civil Rights & Title IX Map Safety & Security Transportation Document Converter Website Requests Back To Top Instagram Facebook LinkedIn TikTok YouTube Copyright © University of Massachusetts Boston UMass System Accessibility Statement Privacy & Terms